{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "FinalNotebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2t2uNJLkswNb",
        "colab_type": "text"
      },
      "source": [
        "# Project Outline\n",
        "\n",
        "1. Import proper packages and open files\n",
        "2. Clean the data\n",
        "3. Create bag of words and lemmatize\n",
        "4. Vectorize into one, two, and three word phrases\n",
        "5. Implementing the Model\n",
        "\n",
        "Then, implement the model on the judging data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iVkcwgJswNc",
        "colab_type": "text"
      },
      "source": [
        "## Step One: Import proper packages and open files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "946-ZBMPswNd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "8568f022-8764-45ec-f2f7-89e2b6de3838"
      },
      "source": [
        "# important packages to work with\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "# NLP packages to clean the data, lemmatize, and vectorize\n",
        "import nltk                                                                     \n",
        "import re\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords                                 # used to clean data\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem.wordnet import WordNetLemmatizer                   # used to lemmatize\n",
        "nltk.download('wordnet')\n",
        "from sklearn.feature_extraction.text import CountVectorizer       # used to create feature vectors\n",
        "\n",
        "# sklearn packages for machine learning\n",
        "from sklearn.model_selection import train_test_split              # used to create train-test split             \n",
        "from sklearn.linear_model import LogisticRegression               # used to implement Logistic Regression\n",
        "from sklearn.metrics import accuracy_score                        # used to get accuracy\n",
        "\n",
        "# tensorflow packages for machine learning\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential                    # used to create the model\n",
        "from tensorflow.keras.layers import Activation, Dense             # used to add layers to the neural net\n",
        "from tensorflow.keras.optimizers import Adam                      # optimizer    \n",
        "# used to prevent overfitting\n",
        "from tensorflow.keras.callbacks import EarlyStopping \n",
        "from tensorflow.keras.layers import Dropout                       "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryubT4zs1t6O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7afc5f59-1f93-4346-bba0-8c5cadb81b82"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlGXDbrT1ob5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv(\"/content/gdrive/My Drive/High School/Summer 2020/Ignition Hacks 2020/training_data.csv\")\n",
        "judging = pd.read_csv(\"/content/gdrive/My Drive/High School/Summer 2020/Ignition Hacks 2020/contestant_judgment.csv\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC5p6JphswNk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "5779c322-3099-4cef-f01f-18cdd26aeffd"
      },
      "source": [
        "# Taking first look at the data\n",
        "train.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>User</th>\n",
              "      <th>Text</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>864192</td>\n",
              "      <td>Carly_FTS</td>\n",
              "      <td>I *heart* filling up @dennisschaub desk   1 it...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>523691</td>\n",
              "      <td>Open_Sourcing</td>\n",
              "      <td>#SocioMat - people create prettier, younger an...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>584154</td>\n",
              "      <td>xxcharlx</td>\n",
              "      <td>no way i dont want the tour to end</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1527961</td>\n",
              "      <td>andreapuddu</td>\n",
              "      <td>@HemalRadia Hi Amazing Brother! Sending Limitl...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>28609</td>\n",
              "      <td>umbec</td>\n",
              "      <td>@flockmaster they are chocolate</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        ID  ... Sentiment\n",
              "0   864192  ...         1\n",
              "1   523691  ...         1\n",
              "2   584154  ...         0\n",
              "3  1527961  ...         1\n",
              "4    28609  ...         1\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2U8U-qzlaX_P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "4e76a6a2-49aa-4ead-964e-fa27bb6b4713"
      },
      "source": [
        "# Information about the data\n",
        "train.info()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000000 entries, 0 to 999999\n",
            "Data columns (total 4 columns):\n",
            " #   Column     Non-Null Count    Dtype \n",
            "---  ------     --------------    ----- \n",
            " 0   ID         1000000 non-null  int64 \n",
            " 1   User       1000000 non-null  object\n",
            " 2   Text       1000000 non-null  object\n",
            " 3   Sentiment  1000000 non-null  int64 \n",
            "dtypes: int64(2), object(2)\n",
            "memory usage: 30.5+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAdTGxLAswNw",
        "colab_type": "text"
      },
      "source": [
        "## Step Two: Clean the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXRn11KmswNx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# stop_words contains typical sentence stopping words from the english language\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# contains all the filtered (good) words\n",
        "filtered_sent = []\n",
        "\n",
        "for index, row in train.iterrows():\n",
        "    tokenized_words = []\n",
        "    # Remove @s\n",
        "    train.at[index, 'Text'] = re.sub(r\"@[A-z]+\", \" \", train.at[index, 'Text'])\n",
        "    \n",
        "    # Remove punctuation\n",
        "    train.at[index, 'Text'] = re.sub(r\"\\W\", \" \", train.at[index, 'Text'])\n",
        "\n",
        "    # Remove numbers\n",
        "    train.at[index, 'Text'] = re.sub(r\"\\d+\", \"\", train.at[index, 'Text'])\n",
        "\n",
        "    # Remove spaces\n",
        "    train.at[index, 'Text'] = re.sub(r\"\\s+\", \" \", train.at[index, 'Text'])\n",
        "   \n",
        "    # Make everything lowercase\n",
        "    train.at[index, 'Text'] = str(train.at[index, 'Text']).lower()\n",
        "    \n",
        "    # Tokenize\n",
        "    tokenized_words = nltk.word_tokenize(train.at[index, 'Text'])\n",
        "    \n",
        "    # Appending all the filtered words to filtered_sent\n",
        "    filtered_sent.append([])\n",
        "    for w in tokenized_words:\n",
        "        if w not in stop_words:\n",
        "            filtered_sent[index].append(w)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pe-ir1hVswN3",
        "colab_type": "text"
      },
      "source": [
        "## Step Three: Create Bag of Words and Lemmatize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-a8PMifswN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Getting lemmatizer from nltk library\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# looping through filtered_sent and lemmatizing each of the words\n",
        "# lemmatizing means to sort words with similar stem or form\n",
        "i = 0\n",
        "for words in filtered_sent:\n",
        "    newWords = []\n",
        "\n",
        "    # Lemmatizing the words in each list of filtered words\n",
        "    for word in words:\n",
        "        newWords.append(lemmatizer.lemmatize(word, pos = 'v'))\n",
        "     \n",
        "    # putting the lemmatized words back in the filtered_send list\n",
        "    filtered_sent[i] = \" \".join(newWords)\n",
        "    i += 1"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENtaoZ50swN7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ace1b921-19ce-4c8b-d410-824e98009f95"
      },
      "source": [
        "# checking if lemmatization worked\n",
        "\n",
        "print(filtered_sent[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "heart fill desk mean sales amp desk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8ZL5g4IswN-",
        "colab_type": "text"
      },
      "source": [
        "## Step Four: Vectorize into one and two-word phrases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ODik1ASswN_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "184b582a-e4ff-4a2e-8484-85592a3f7a77"
      },
      "source": [
        "# creating a new column in the dataframe with the lemmatized bag of words\n",
        "new_column = pd.DataFrame({'BagofWords': filtered_sent})\n",
        "train = train.merge(new_column, right_index = True, left_index=True)\n",
        "train.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>User</th>\n",
              "      <th>Text</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>BagofWords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>864192</td>\n",
              "      <td>Carly_FTS</td>\n",
              "      <td>i heart filling up desk it means sales amp it ...</td>\n",
              "      <td>1</td>\n",
              "      <td>heart fill desk mean sales amp desk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>523691</td>\n",
              "      <td>Open_Sourcing</td>\n",
              "      <td>sociomat people create prettier younger and b...</td>\n",
              "      <td>1</td>\n",
              "      <td>sociomat people create prettier younger better...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>584154</td>\n",
              "      <td>xxcharlx</td>\n",
              "      <td>no way i dont want the tour to end</td>\n",
              "      <td>0</td>\n",
              "      <td>way dont want tour end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1527961</td>\n",
              "      <td>andreapuddu</td>\n",
              "      <td>hi amazing brother sending limitless love you...</td>\n",
              "      <td>1</td>\n",
              "      <td>hi amaze brother send limitless love way twitt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>28609</td>\n",
              "      <td>umbec</td>\n",
              "      <td>they are chocolate</td>\n",
              "      <td>1</td>\n",
              "      <td>chocolate</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        ID  ...                                         BagofWords\n",
              "0   864192  ...                heart fill desk mean sales amp desk\n",
              "1   523691  ...  sociomat people create prettier younger better...\n",
              "2   584154  ...                             way dont want tour end\n",
              "3  1527961  ...  hi amaze brother send limitless love way twitt...\n",
              "4    28609  ...                                          chocolate\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdH5HeFGswOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating original features, next step is vectorization\n",
        "bagofwords = train.BagofWords.tolist()\n",
        "\n",
        "# creating labels\n",
        "sentiment = train.Sentiment.tolist()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAh1XcOQswOY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "8e3752ca-5397-4f1c-8205-c8ffeec76e94"
      },
      "source": [
        "# creating a CountVectorizer for 400 single-word phrases\n",
        "cv = CountVectorizer(max_features=400)\n",
        "\n",
        "# creating a CountVectorizer for 400 two or three-word phrases phrases\n",
        "cv2 = CountVectorizer(max_features=400, ngram_range=(2, 3))\n",
        "\n",
        "# concatenating the arrays that have formed small vectors to get a large feature vector\n",
        "x = np.concatenate((cv.fit_transform(train['BagofWords']).toarray(), cv2.fit_transform(train['Text']).toarray()), axis=1)\n",
        "\n",
        "# labels\n",
        "y = train['Sentiment'].values"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n\\n# getting the feature names from the count vectorizer\\nheader_1 = cv.get_feature_names()\\nheader_2 = cv2.get_feature_names()\\n\\n# creating two output dataframes for both count vectorizers\\noutput_1 = pd.DataFrame(x_1, columns = header_1)\\noutput_2 = pd.DataFrame(x_2, columns = header_2)\\n\\n# merging the two dataframes into one\\noutput_1 = output_1.merge(output_2, right_index=True, left_index=True)\\n\\n# adding the labels column to the dataframe\\noutput_1['Sentiment'] = train['Sentiment']\\n\\noutput_1.head()\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u1nPDMhtVuD",
        "colab_type": "text"
      },
      "source": [
        "# Step 5: Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xzvd8w17tXw4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# splitting the data into train and test using sklearn\n",
        "# train: 75%, test: 25%\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.25)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FHF6Nio2niR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "3caeed88-34a1-443f-f412-52753ed89462"
      },
      "source": [
        "# Implementing a basic ML model (logistic regression)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier = LogisticRegression(max_iter=700)\n",
        "\n",
        "# fitting the classifier on X_train and y_train\n",
        "\n",
        "classifier.fit(X_train, y_train)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=700,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXYdZO9R2rxT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5fd76b4e-302f-4194-ed92-a0830b71074c"
      },
      "source": [
        "# making predictions with the classifier on X_test\n",
        "predictions = classifier.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# using sklearn to get the accuracy (since this is a classification task  )\n",
        "accuracy_score(y_test, predictions)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.737308"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BE3gRLN-Uzh",
        "colab_type": "text"
      },
      "source": [
        "#### Creating an Artificial Neural Network with Tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THko5Bk13l-n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "1adf171f-9f68-486e-c062-452d38b2151d"
      },
      "source": [
        "# Splitting the train data into train and validation sets\n",
        "# validation loss is constantly being checked to prevent overfitting\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
        "\n",
        "# Using model = Sequential() to add layers to our neural net\n",
        "model = Sequential()\n",
        "\n",
        "# Input layer: input shape is (800,) and activation function is relu\n",
        "model.add(Dense(units=256, input_shape=X_train[0].shape, activation='relu'))\n",
        "\n",
        "# hidden layers: no activation functions\n",
        "model.add(Dense(units=128, activation=None))\n",
        "model.add(Dense(units=64, activation=None))\n",
        "model.add(Dense(units=48, activation=None))\n",
        "model.add(Dense(units=32, activation=None))\n",
        "model.add(Dense(units=16, activation=None))\n",
        "model.add(Dense(units=12, activation=None))\n",
        "model.add(Dense(units=8, activation=None))\n",
        "model.add(Dense(units=4, activation=None))\n",
        "\n",
        "# output layer: sigmoid activation function\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# compiling the model\n",
        "# adam optimizer, binary crossentropy loss\n",
        "# also will allow us to see the accuracy while the model is being trained\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# early_stopping monitors validation loss to prevent overfitting\n",
        "early_stop = EarlyStopping(monitor='val_loss')\n",
        "\n",
        "# printing out the summary of the model with all the layers\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 256)               205056    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 48)                3120      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 32)                1568      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 12)                204       \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 8)                 104       \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 4)                 36        \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1)                 5         \n",
            "=================================================================\n",
            "Total params: 251,773\n",
            "Trainable params: 251,773\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Rd1wXz03uvH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "17c18cf3-91e5-470e-c80a-c93a87deff24"
      },
      "source": [
        "# Training the model on the train data\n",
        "# 10 epochs with a validation split of 0.25\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=10, verbose=1, validation_split=0.25)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "17579/17579 [==============================] - 54s 3ms/step - loss: 0.5255 - accuracy: 0.7340 - val_loss: 0.5144 - val_accuracy: 0.7408\n",
            "Epoch 2/10\n",
            "17579/17579 [==============================] - 55s 3ms/step - loss: 0.5031 - accuracy: 0.7494 - val_loss: 0.5155 - val_accuracy: 0.7458\n",
            "Epoch 3/10\n",
            "17579/17579 [==============================] - 54s 3ms/step - loss: 0.4846 - accuracy: 0.7619 - val_loss: 0.5075 - val_accuracy: 0.7467\n",
            "Epoch 4/10\n",
            "17579/17579 [==============================] - 55s 3ms/step - loss: 0.4641 - accuracy: 0.7743 - val_loss: 0.5247 - val_accuracy: 0.7437\n",
            "Epoch 5/10\n",
            "17579/17579 [==============================] - 56s 3ms/step - loss: 0.4440 - accuracy: 0.7860 - val_loss: 0.5360 - val_accuracy: 0.7385\n",
            "Epoch 6/10\n",
            "17579/17579 [==============================] - 55s 3ms/step - loss: 0.4257 - accuracy: 0.7965 - val_loss: 0.5581 - val_accuracy: 0.7365\n",
            "Epoch 7/10\n",
            "17579/17579 [==============================] - 55s 3ms/step - loss: 0.4093 - accuracy: 0.8058 - val_loss: 0.5644 - val_accuracy: 0.7317\n",
            "Epoch 8/10\n",
            "17579/17579 [==============================] - 55s 3ms/step - loss: 0.3946 - accuracy: 0.8136 - val_loss: 0.5764 - val_accuracy: 0.7292\n",
            "Epoch 9/10\n",
            "17579/17579 [==============================] - 55s 3ms/step - loss: 0.3816 - accuracy: 0.8205 - val_loss: 0.5824 - val_accuracy: 0.7270\n",
            "Epoch 10/10\n",
            "17579/17579 [==============================] - 57s 3ms/step - loss: 0.3697 - accuracy: 0.8261 - val_loss: 0.6531 - val_accuracy: 0.7248\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mDnnPCD5FNA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = model.predict(X_test)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn8ZKIBG5uga",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3d162852-856a-42a1-d61a-b2f549fbb983"
      },
      "source": [
        "len(preds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5MWV8z8B3RW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = []\n",
        "\n",
        "for i in range(len(preds)):\n",
        "  if preds[i] >= 0.5:\n",
        "    predictions.append(1)\n",
        "  else:\n",
        "    predictions.append(0)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_TfFUvaCG4R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4531cf7a-3bcc-4507-b3f9-829c4deac1c8"
      },
      "source": [
        "accuracy_score(y_test, predictions)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.724508"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoaAb3KLqwiv",
        "colab_type": "text"
      },
      "source": [
        "# Make Predictions on the Judging Dataframe\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHCW6SX9sV59",
        "colab_type": "text"
      },
      "source": [
        "#### Doing the same pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXS7dXokq5K9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# stop_words contains typical sentence stopping words from the english language\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# contains all the filtered (good) words\n",
        "filtered_sent = []\n",
        "\n",
        "for index, row in judging.iterrows():\n",
        "    tokenized_words = []\n",
        "    # Remove @s\n",
        "    judging.at[index, 'Text'] = re.sub(r\"@[A-z]+\", \" \", judging.at[index, 'Text'])\n",
        "    \n",
        "    # Remove punctuation\n",
        "    judging.at[index, 'Text'] = re.sub(r\"\\W\", \" \", judging.at[index, 'Text'])\n",
        "\n",
        "    # Remove numbers\n",
        "    judging.at[index, 'Text'] = re.sub(r\"\\d+\", \"\", judging.at[index, 'Text'])\n",
        "\n",
        "    # Remove spaces\n",
        "    judging.at[index, 'Text'] = re.sub(r\"\\s+\", \" \", judging.at[index, 'Text'])\n",
        "   \n",
        "    # Make everything lowercase\n",
        "    judging.at[index, 'Text'] = str(judging.at[index, 'Text']).lower()\n",
        "    \n",
        "    # Tokenize\n",
        "    tokenized_words = nltk.word_tokenize(judging.at[index, 'Text'])\n",
        "    \n",
        "    # Appending all the filtered words to filtered_sent\n",
        "    filtered_sent.append([])\n",
        "    for w in tokenized_words:\n",
        "        if w not in stop_words:\n",
        "            filtered_sent[index].append(w)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsbjRGtWsZqP",
        "colab_type": "text"
      },
      "source": [
        "#### Doing the same lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_-bsrWsq-JJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Getting lemmatizer from nltk library\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# looping through filtered_sent and lemmatizing each of the words\n",
        "# lemmatizing means to sort words with similar stem or form\n",
        "i = 0\n",
        "for words in filtered_sent:\n",
        "    newWords = []\n",
        "\n",
        "    # Lemmatizing the words in each list of filtered words\n",
        "    for word in words:\n",
        "        newWords.append(lemmatizer.lemmatize(word, pos = 'v'))\n",
        "     \n",
        "    # putting the lemmatized words back in the filtered_send list\n",
        "    filtered_sent[i] = \" \".join(newWords)\n",
        "    i += 1"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ot_m-ci0rFSI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "judging['BagofWords'] = filtered_sent"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0s4VjTyssy7",
        "colab_type": "text"
      },
      "source": [
        "#### Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2XcMTo1rLZV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating original features, next step is vectorization\n",
        "bagofwords = judging.BagofWords.tolist()"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jMvnKkorRHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating a CountVectorizer for 400 single-word phrases\n",
        "cv = CountVectorizer(max_features=400)\n",
        "\n",
        "# creating a CountVectorizer for 400 two or three-word phrases phrases\n",
        "cv2 = CountVectorizer(max_features=400, ngram_range=(2, 3))\n",
        "\n",
        "# concatenating the arrays that have formed small vectors to get a large feature vector\n",
        "x = np.concatenate((cv.fit_transform(judging['BagofWords']).toarray(), cv2.fit_transform(judging['Text']).toarray()), axis=1)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyJJMT5Ns2Yq",
        "colab_type": "text"
      },
      "source": [
        "#### Making the predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZVO58l-rd1o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_predictions = classifier.predict(x)\n",
        "\n",
        "for i in range(len(test_predictions)):\n",
        "  if test_predictions[i] >= 0.5:\n",
        "    test_predictions[i] = 1\n",
        "  else:\n",
        "    test_predictions[i] = 0"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLpxAyH4rhL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_nn_predictions = model.predict(x)\n",
        "\n",
        "for i in range(len(test_nn_predictions)):\n",
        "  if test_nn_predictions[i] >= 0.5:\n",
        "    test_nn_predictions[i] = 1\n",
        "  else:\n",
        "    test_nn_predictions[i] = 0"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9X3X4I5Esy9f",
        "colab_type": "text"
      },
      "source": [
        "#### Put the predictions into the final\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08guv8uKs4AV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "judging['Sentiment'] = test_predictions"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nxe27-y6tPVl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "judging.to_csv('judging.csv')\n",
        "!cp judging.csv \"gdrive/My Drive/High School/Summer 2020/Ignition Hacks 2020/judging.csv\""
      ],
      "execution_count": 41,
      "outputs": []
    }
  ]
}